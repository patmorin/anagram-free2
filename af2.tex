\documentclass{patmorin}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,wrapfig}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{amsthm,mathtools}
\usepackage{pat}
\usepackage{paralist}
\usepackage{stmaryrd}
\usepackage[noend,]{algorithmic}
% \usepackage{thm-restate}

% \usepackage[longnamesfirst,numbers,sort&compress]{natbib}
% \usepackage[noabbrev,capitalise]{cleveref}

\usepackage{doi} % To make plainnat handle doi's properly 

\newcommand{\ceil}[1]{\lceil #1\rceil}
% \newcommand{\floor}[1]{\lfloor #1\rfloor}

\usepackage[mathlines]{lineno}
% \setlength{\linenumbersep}{2em}
% \linenumbers
% \rightlinenumbers
% \linenumbers
\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
 \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
 \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
 \renewenvironment{#1}%
    {\linenomath\csname old#1\endcsname}%
    {\csname oldend#1\endcsname\endlinenomath}}% 
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
 \patchAmsMathEnvironmentForLineno{#1}%
 \patchAmsMathEnvironmentForLineno{#1*}}%
\AtBeginDocument{%
\patchBothAmsMathEnvironmentsForLineno{equation}%
\patchBothAmsMathEnvironmentsForLineno{align}%
\patchBothAmsMathEnvironmentsForLineno{flalign}%
\patchBothAmsMathEnvironmentsForLineno{alignat}%
\patchBothAmsMathEnvironmentsForLineno{gather}%
\patchBothAmsMathEnvironmentsForLineno{multline}%
}

\setlength{\parskip}{1ex}

\newcommand{\floor}[1]{\lfloor #1\rfloor}

\title{\MakeUppercase{Anagram-Free Sequences via Entropy Compression}}
\author{The Armenian Gang}




\begin{document}
\maketitle

\begin{abstract}
  We show that arbitrarily long anagram-free sequences (also known as strongly-non-repetitive sequences) over a small alphabet can be generated using a simple randomized algorithm.
\end{abstract}

\section{Introduction}

A string $s_1,\ldots,s_{2k}$ is an \emph{anagram} if $s_1,\ldots,s_k$ is a permutation of $s_{k+1},\ldots,s_{2k}$.  A string $s_1,\ldots,s_n$ is \emph{anagram-free} if it contains no (consecutive) substring that is an anagram.  We say that a string is $\ge k$-anagram-free if it contains no substring of length at least $k$ that is an anagram.

Consider the following algorithm for generating $\ge k$-anagram-free strings over an alphabet $\Sigma$ with $|\Sigma|=c$:

\noindent$\textsc{Anagram-Free}(m, k, \Sigma)$:
\begin{algorithmic}[1]
  \STATE{$n\gets 0$}
  \FOR{$i\gets 1,\ldots,m$}
    \STATE{$x_i \gets \text{a uniformly random element in $\Sigma$}$}
    \STATE{$s_{n+1}\gets x_i$}
    \STATE{$n\gets n+1$}
    \STATE{Let $t$ be the largest integer such that $s_{n-2t+1},\ldots,s_n$ is an anagram}
    \STATE{$n\gets n-t$}
  \ENDFOR
  \RETURN{$s_1,\ldots,s_n$}
\end{algorithmic}

The preceding algorithm clearly returns an anagram free string.  We wish to show that, for sufficiently large $\Sigma$, $\textsc{Anagram-Free}(m, k, \Sigma)$ (typically) returns a string whose length, $n$, is linear in the number, $m$, of iterations.

Note that, when the algorithm returns in Line~8, it returns a string of length
\begin{equation}  
  n = m - \sum_{i=1}^m t_i  \eqlabel{sumsize}
\end{equation}
since each iteration $i\in\{1,\ldots,m\}$ increments $n$ (Line~5) and decreases $n$ by $t_i$ (Line~7) during iteration $i$.

\section{The Encoding Scheme}

We now describe an encoding scheme that traces the execution of $\textsc{Anagram-Free}(m, k, \Sigma)$ and outputs a sequence of bits that allows us to recover the random string $x_1,\ldots,x_m$.  If the algorithm produces an output of length $n < \epsilon m$ then the length of the encoding will be less than $m\log c$.  Since $x_1,\ldots,x_m$ is uniformly random over $|\Sigma|^m$---a set of size $c^m$---the probability that it can be encoded using $m\log c - k$ or fewer bits is at most $2^{-k}$.

We use the phrase ``encoding scheme'' loosely here.  In particular, we will make use of ``codes'' where codewords have non-integer lengths, sometimes even having lengths much less than 1.  We are only doing this for the sake of analysis and all our results could be restated in terms of probabilities, though possibly less intuitively.

We say that a sequence of positive numbers $\langle \ell_t : t\in\Z_{\ge 0}\rangle$ satisfies \emph{Kraft's Inequality} if $\sum_{t=0}^{\infty} 2^{-\ell_t} \le 1$.  In this work, we will use codes for non-negative integers in which the ``codeword'' for each $t\in\Z_{\ge 0}$ has length $\ell_t$. Although non-intuitive, this can be rigorously justified so long as the codeword lengths satisfy Kraft's Inequality \cite[Section~7]{morin.mulzer.ea:encoding}.

\subsection{The Deletion-Length Code}

Critical to this work is a code for non-negative integers whose codeword lengths $\langle \ell_t:t\in\Z_{\ge0}\rangle$ are carefully balanced in order to satisfy Kraft's Inequality in addition to some other requirements.  In this code, the codeword for 0 has length
\[
     \ell_0 = \log\left(\frac{c}{c-1}\right)+\frac{\beta\log e}{c}
       \approx \frac{(1+\beta)\log e}{c}
\]
for some $0 < \beta <1$ that will be discussed later.  For each $t\in \Z_{\ge0}$, we let $w_t=2^{\ell_t}$, so that Kraft's Inequality requires $\sum_{t=0}^\infty w_t^{-1} \le 1$.  We have
\[
   w_0^{-1} = 2^{-\ell_0} \approx 2^{-\tfrac{(1+\beta)\log e}{c}} = e^{-(1+\beta)/c} \approx 1-(1+\beta)/c \enspace .
\]
Therefore, in order to satisfy Kraft's Inequality, we must choose codeword lengths that satisfy
\[
   \sum_{t=1}^\infty 2^{-\ell_t} < 1-w_0^{-1} \approx (1+\beta)/c \enspace .
\]
Note that this implies that the codeword length $\ell_t$, for every $t\in\N$, must be quite large.  Indeed, satisfying Kraft's Inequality requires $\ell_t > \log c - \log(1+\beta)$.

% To begin with, we choose the codeword-length for $1$ to be
% \[
%     \ell_1 = \log c + (1-\epsilon)c\ell_0
% \]
% so
% \[
%     w_1^{-1} = 2^{-\ell_1} 
%     = \frac{2^{-(1-\epsilon)c\ell_0}}{c}
%     = \frac{e^{(1+\beta)(\epsilon-1)}}{c}  \enspace .
% \]
% To satisfy Kraft's Inequality we must select codeword lengths that satisfy
% \[
%     \sum_{t=2}^\infty 2^{-\ell_t} < 1-w_0^{-1}-w_1^{-1}
%     = 1-e^{-(1+\beta)/c}-\frac{e^{(1+\beta)(\epsilon-1)}}{c}
%     \approx \frac{(1+\beta) - e^{-(1+\beta)}}{c}
% \]
% where $\approx$ denotes the behaviour of the expression as $c\rightarrow\infty$ and $\epsilon\rightarrow 0$.  For $\beta$ close to 1 this function is positive, and for $\beta>0.91$, this function is greater than $1/2$.

\subsection{Encoding}

Next we describe the encoding scheme.  To encode $c_1,\ldots,c_m$, we encode the following information:
\begin{enumerate}
  \item The string $s_1,\ldots,s_n$ returned by algorithm in Line~8. Since this string is anagram free, $s_{i+1}\neq s_{i}$ for any $i\in\{1,\ldots,n\}$, so this can be done using $\log(c) + (n-1)\log(c-1)$ bits.
  \item The sequence $t_1,\ldots,t_m$ of deletion-lengths using $\sum_{i=1}^m \ell_{t_i}$ bits.
  \item A sequence $\xi_1,\ldots,\xi_m$ where each $\xi_i$ contains enough information to recover the $t_i$ values deleted from $s$ during iteration $i$.
\end{enumerate}

The number of bits required for the third item is the subject of \secref{something}, where we will show that the number of bits required for $\xi_i$ is at most
\begin{equation}
   |\xi_i| 
   \le \begin{cases}
     0 & \text{if $t_i = 0$} \\
     t_i\log c +(\beta/c)\log e + (t_i-1)\log(c-1) - t_i(\alpha/c)\log e) - \ell_{t_i} 
        & \text{otherwise} \\
     t_i(\log c - (\alpha/c)\log e) +(\beta/c)\log e - \ell_{t_i} 
        & \text{otherwise}
   \end{cases}  \eqlabel{xi-i}
\end{equation}
for some $1 > \alpha > \beta$.
Here is the intuition behind this choice.  Infortmation-theoretically, each iteration $i\in\{1,\ldots,m\}$ uses one random value $x_i$, so it should contribute at least $\log c$ bits to the length of the encoding.  In fact, each iteration will contribute $\log c + (\beta/c)\log e$ bits to the encoding.  Thus, we should think of $(\beta/c)\log e$ as an overhead cost per iteration so that the total overhead is $m(\beta/c)\log n$.  However, if $t_i>0$, then $(\alpha/c)t_i$ bits of this overhead are recovered.  Thus, $\sum_{i=1}^m t_i(\alpha/c)\log e = (m-n)(\alpha/c)\log e$ bits.  Thus, the total length of the encoding is
\[
    m\log c + m(\beta/c)\log e - (m-n)(\alpha/c)\log e < m\log c
\]
when $n<(1-\beta/\alpha)m$.

We make this more formal as follows:  For each $t\in\N$, let $d_t$ denote the number of iterations $i\in\{1,\ldots,m\}$ for which $t_i=t$. 
\begin{align*}
    |C| 
    & = \log c + (n-1)\log(c-1) + d_0\ell_0 \\ 
      & \quad {} + \sum_{t=1}^m \left(d_t t(\log c - (\alpha/c)\log e) + (\beta/c)\log e\right) \\
    & = \log c + (n-1)\log(c-1) + d_0(\log(c/(c-1))+(\beta/c)\log e) \\ 
      & \quad {} + \sum_{t=1}^m \left(d_t t(\log c - (\alpha/c)\log e) + (\beta/c)\log e\right) \\
    & = \log c + (n-1)\log(c-1) + m(\beta/c)\log e + d_0\log(c/(c-1) \\ 
      & \quad {} + \sum_{t=1}^m \left(d_t t(\log c - (\alpha/c)\log e) \right) \\
    & = \log c + (n-1)\log(c-1) + m(\log c + (\beta/c)\log e) - d_0\log(c-1) \\
      & \quad {} - \sum_{t=1}^m \left(d_t t(\alpha/c)\log e \right) \\
    & = \log c + (n-1)\log(c-1) + m(\log c + (\beta/c)\log e) - d_0\log(c-1) \\ 
      & \quad {} - (m-n)(\alpha/c)\log e \\
    & \le \log (c/(c-1)) + m(\log c + (\beta/c)\log e) - (m-n)(\alpha/c)\log e \\
    & < m\log c - k
\end{align*}
when $n \le BLAH(k)$.  This shows that the probability that the algorithm fails to produce an output of length at least $BLAH$ is at most $2^{-k}$.

It still remains to describe the codeword lengths $\langle \ell_t:i\in \N\rangle$ and encodings of $\xi_i$ satisfying \eqref{xi-i}, and to show that these codeword lengths satisfy Kraft's Inequality.  Before doing so, we first review how the encoding can be used to recover $x_1,\ldots,x_m$.


\subsection{Decoding}

To recover $x_1,\ldots,x_m$ using this information, we proceed as follows:
\begin{enumerate}
  \item If $t_i=0$ for all $i\in\{1,\ldots,m\}$, then $m=n$ and we immediately deduce that $x_1,\ldots,x_m=s_1,\ldots,s_n$.
  \item Otherwise, we find the minimum value $d$ such that $t_{m-d}\neq 0$.  
  From this we deduce that $x_{m-d+1},\ldots,x_m = s_{n-d+1},\ldots,s_n$.
  \item We now know that $s_1,\ldots,s_{n-d}$ was the string generated by the algorithm at the end of iteration $i$, immediately after deleting a suffix $s'_{n-d+1},\ldots,s'_{n-d+t_{m-d}}$ of length $t_{m-d}$.
  \item The string $\xi_{m-d}$ along with $s_1,\ldots,s_{n-d}$ contains enough information to recover $s'_{n-d+1},\ldots,s'_{n-d+t_{m-d}}$.
  \item We know that $x_{m-d} = s'_{n-d+t_{m-d}}$ triggered the deletion of $t_{m-d}$ elements.
  \item Therefore, at the end of iteration $m-d-1$ the algorithm had constructed the string $s_1,\ldots,s_{n-d},s'_{n-d+1},\ldots,s'_{n-d+t_{m-d}-1}$.
  \item At this point we have recovered $x_{m-d},\ldots,x_m$ as well as the string constructed by the algorithm at the conclusion of iteration $m-d-1$.  We can the proceed inductively using $t_1,\ldots,t_{m-d-1}$, $\xi_1,\ldots,\xi_{m-d-1}$ and $s_1,\ldots,s_{n-d},s'_{n-d+1},\ldots,s'_{n-d+t_{m-d}-1}$ to recover $x_1,\ldots,x_{m-d-1}$.
\end{enumerate}

\subsection{Code Length}

We now describe the rest of the code.  For an anagram-free string $s=s_1,\ldots,s_t\in \Sigma^{t}$, let $a(s)$ be the number of strings $s_{t+1},\ldots,s_{2t}\in\Sigma^t$ such that, (i)~$s_1,\ldots,s_{t+i}$ is anagram-free for all $i\in\{1,\ldots,t-1\}$ and (ii)~$s_1,\ldots,s_{2t}$ is an anagram.  Let 
\[
   a_t = \max\left|\left\{a(s) : \text{$s\in \Sigma^t$ and $s$ is anagram-free} \right\}\right|
\]
Observe that the length of the codeword needed for $\xi_i$ is at most $\log a_{t_i}$.  Therefore, \eqref{xi-i} is satisfied by choosing, for each $t\in \N$,
\[
   \ell_t = t\log c - (\alpha/c)\log e) + (\beta/c)\log e - \log a_t   \enspace .
\]
All that remains is to show that this leads to a sequence of codeword lengths $\langle \ell_t : i\in Z_{\ge 0}\rangle$ that satisfy Kraft's Inequality.  In particular, we need to show that
\[
  \sum_{t=1}^\infty 2^{-\ell_t} = \sum_{t=1}^\infty 2^{-\ell_t} \le 1-e^{-\beta/c} \approx \beta\log c/c \enspace .
\]

\subsection{Analysis of $a_t$ for $t\le c$: Irreducible Permutations}

Beginning with $a_1$, we see that $a_1=1$ since any anagram of length 2 has its second half equal to its first half.  Therefore, $a_1=\log 1 = 0$, so $\ell_1=\log c - \ell_0$ and 
\[
   w_1^{-1} = 2^{-\ell_1} = \frac{w_0}{c} = \frac{e^{\beta/c}}{c} \approx \frac{1}{c}+\frac{\beta}{c^2}
\]
Since $\beta >1$, this still leaves some wiggle-room:
\[
    w_0^{-1} + w_1^{-1} = e^{-\beta/c} + \frac{e^{\beta/c}}{c}
    \le 1 - \frac{\beta}{c} + O(1/c^2)  \quad \text{Double check!!}
\]

For $a_2$, a quick check verifies that if $s_1,\ldots,s_3$ is anagram-free but $s_1,\ldots,s_4$ is an anagram, then $s_1s_2=s_3s_4$.  Therefore $a_2=1$, so
$\ell_2 = 2(\log c - \ell_0)$ and
\[
   w_2^{-1} = \left(\frac{w_0}{c}\right)^2 < \left(\frac{2}{c}\right)^2 \enspace , 
\]
so 
\[
  \sum_{t=0}^2 w_t^{-1} = \frac{\beta}{c} + O(1/c^2) \enspace .
\]

The value of $a_3$ represents the first interesting case.  Again, if $s_1,\ldots,s_5$ is anagram-free but $s_1,\ldots,s_6$ is an anagram, then $s_1,s_2,s_3$ are distinct and $s_4\neq s_3$, and $\{s_4,s_5\}\neq \{s_2,s_3\}$.  This leaves three possibilities for $s_4,s_5,s_6$:
\[   s_1,s_2,s_3 \quad s_1,s_3,s_2 \quad s_2,s_1,s_3 \enspace . \]
Thus, $\ell_3 = 3(\log c - \ell_0) - \log 3$ and
\[
   w_3^{-1} = 3\left(\frac{w_0}{c}\right)^{3} \enspace .
\]

The case of $a_3$ illustrates a general phenomenon: For $t\in\{1,\ldots,c\}$, $a(s_1,\ldots,s_t)$ is maximized when $s_1,\ldots,s_t$ are all distinct.  In this case, the requirement that $s_1,\ldots,s_{2t-1}$ is anagram-free implies that $\{s_{t+1},\ldots,s_{t+r}\}\neq\{s_{t-r+1},\ldots,s_t\}$.  Without loss of generality, we may assume that $s_1,\ldots,s_t=t,t-1,\ldots,1$.  Thus, $s_{t+1},\ldots,s_{2t}$ is a permutation $\pi_t,\ldots,\pi_t$ of $\{1,\ldots,t\}$ for which $\{\pi_1,\ldots,\pi_j\}\neq\{1,\ldots,j\}$ for any $j\in\{1,\ldots,t-1\}$.  Such permutations are called \emph{irreducible permutations} and are well-studied.  The sequence $\langle m_t: t\in\N\rangle$ where $m_t$ is the number of irreducible permutations of length $t$ appears as sequence A003319 in the OEIS.  More importantly, there is a recursive formula for $s_t$:
\[
   m_t = t! - \sum_{i=1}^{t-1} m_i(t-i)!
\]
that allows us to efficiently compute exact values of $m_t$. We use this fact in the computer part of our proof.  For now, we will summarize this part as
\[
    \ell_t \ge t(\log c-\ell_0) - \log m_t
\]
so
\[
   w_t^{-1} \le \frac{m_t e^{t\beta\log c/c}}{c^t}
\]
for all $t\in\{1,\ldots,c\}$.  The upper bound $a_t \le s_t$ is most useful for small values of $t$.  For larger values of $t$, the upper bound $a_t \le t!$ is almost as good since $a_t/t!\rightarrow 1$ as $t\rightarrow\infty$.

\subsection{Analysis of $a_t$ for $t > c$: Multinomial Coefficients}

For $t>c$, we observe that the second half of an anagram-free string is a permutation of a multiset.  The number of such distinct permutations is at most
\[
    \frac{t!}{f_1!\times f_2!\times \cdots\times f_t!}
\]
where, for each $i\in\Sigma$, $f_i$ is the number of appearances of $i$ in the first half of the string.  Since the factorial function is convex, this is maximized when $f_1=f_2=\cdots=f_t=t/c$ when $t/c$ is an integer.  When $t/c$ is not an integer, it is maximized when $f_1=f_2=\cdots=f_{t\bmod c}=\ceil{t/c}$ and $f_{1+t\bmod c}=\cdots=f_c=\floor{t/c}$.  Therefore,
\[
    a_t \le \frac{t!}{(\ceil{t/c}!)^{t\bmod c}\times (\floor{t/c}!)^{-t\bmod c}}
    = \frac{t!}{(\floor{t/c}!)^c\times \ceil{t/c}^{t\bmod c}} \enspace .
\]
Using Stirling's Inequality, the preceding upper bound yields a bound of the form
\[
   \log a_t \le t\log t - t\log e + O(\log t) - c\log(\floor{t/c}!) - (t\bmod c)\log(\ceil{t/c}) \enspace .
\]

\subsection{Analysis of $a_t$ for $t\ggg c$: The Dumb Estimate}

As $t$ becomes much larger than $c$, the multinomial estimate from the previous section degenerates to the form
\[
   \log a_t \le t\log c - c\log(t/c) + O(\log t)
\]
In particular, the difference between the trivial code length $t\log c$ and the code length obtained from the estimate increases only logarithmically in $t$.

For $t\ggg c$, we can use estimates based on the fact that the second half of the string is itself anagram-free.  As a basic upper upper-bound, we can start with the observation that, for any string $s_1,\ldots,s_{2t}$ whose second half is deleted, $s_{i}\neq s_{i-1}$ for any $i\in\{t+1,\ldots,2t\}$.  Therefore,
$a_t \le (c-1)^t$ and
\[
     \log a_t \le t\log(c-1) < t\log c - t\left(\frac{\log e}{c-1}\right)
\]
In this way, we get
\begin{align*}
    \ell_t & = t(\log c - \ell_0) - \log a_t \\
        & \ge t\left(\frac{\log e}{c-1}-\ell_0\right) \\
        & \ge t\left(\frac{\log e}{c-1}-\frac{\beta\log c}{c}\right) 
\end{align*}




\end{document}
